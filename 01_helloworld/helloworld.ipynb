{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open-AI is stateless\n",
    "# Open-AI token: Boundary of letters/characters in a text (Like spaces between word in plain text)\n",
    "    # Decided by domain experts\n",
    "    # Open-AI charges us against those tokens (sent via request and received via response)\n",
    "    # When we send a request/input to Open-AI, it charges us on the basis of these tokens\n",
    "    # On an average one token is made of 4 characters.\n",
    "    # Because Open-AI is stateless, we will have to send all the data with each request subsequently number of tokens will become huge and we will have to pay for all the tokens for each request.\n",
    "\n",
    "# To avoid the above issue, Long term memory \"PineCone\" is used.\n",
    "    # Pinecone is a vector data base (vector is an array of number/floating points)\n",
    "    # It converts the profile data into numbers and saves the form of array.\n",
    "    # When we send a request, it responds by fetching the tokens of the same/similar data that was requested.\n",
    "\n",
    "# Embedding: process of converting text into numbers, also called \"vector embedding\" & \"word embedding\"\n",
    "\n",
    "# Chat completion AI has state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to run any command of operating system in Jupyter notebook, we use \"!\" or \"%\"\n",
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## python-dotenv library: \n",
    "* is used for loading environment variables from a file named .env into the system's environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find_dotenv()\n",
    "* This function is part of the python-dotenv library. \n",
    "* It searches for the .env file starting from the current working directory and moving upwards through parent directories until it finds the file. \n",
    "* The purpose is to locate the .env file associated with the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "find_dotenv()   # finds the path to .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load_dotenv()\n",
    "* This function is also part of the python-dotenv library. \n",
    "* It loads the environment variables from the .env file into the Python script's environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv())  # findenv() passes the path to loadenv()\n",
    "                            # after loading every thing becomes variables in operating systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these variables can be extracted by following method\n",
    "import os\n",
    "\n",
    "os.environ['MY_NAME']\n",
    "os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# API keys are not passed directly in our code\n",
    "# instead it is saved in .env file in the form of a variable\n",
    "# these variables names are passed into our code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The use of _ as the variable name\n",
    "* It is a convention in Python when the variable's value is not going to be used later in the code. \n",
    "* It's a way to indicate that the result is intentionally ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI       # imports OpenAI class\n",
    "from dotenv import find_dotenv ,load_dotenv\n",
    "\n",
    "_ : bool = load_dotenv(find_dotenv())\n",
    "\n",
    "client: OpenAI = OpenAI() # this client will be used to communicate with OpenAI LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.chat.chat_completion import ChatCompletion\n",
    "\n",
    "def chat_completion(prompt: str) -> str:                        # takes an argument (prompt)\n",
    "    response: ChatCompletion = client.chat.completions.create(  # client that we created in previous cell\n",
    "                                                                # create() takes 2 arguments, messages & model\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",         # human interacting with LLM\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        model = \"gpt-3.5-turbo-1106\",\n",
    "    )\n",
    "\n",
    "    display(dict(response))\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "chat_completion(\"What is 1+1?\") # This question string will be assigned to \"prompt\" parameter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
