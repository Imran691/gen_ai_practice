{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Calling\n",
    "* Data can be made available to LLM by following 3 methods.\n",
    "    * Providing required data to base model of LLM (by fine tuning)\n",
    "    * By saving the data on a vector data base (in the form of embeddings(numbers))\n",
    "    * Function calling\n",
    "        * Used to get real time, sensitive data (data remains at the user end)\n",
    "        * LLM calls the API provided by the user to provide the answer to the user\n",
    "        * As OperAI is stateless, it has to call the function every time the user asks the same question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "_ : bool = load_dotenv(find_dotenv())\n",
    "\n",
    "client : OpenAI = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_weather(location: str, unit:str='fahrenheit') -> str:\n",
    "    \"\"\"Get current weather in a given location\"\"\"\n",
    "    if \"tokio\" in location.lower():\n",
    "        return json.dumps({\"Location\": \"Tokio\", \"temperature\": \"10\", \"unit\": \"celcius\"})\n",
    "    elif \"san fransisco\" in location.lower():\n",
    "        return json.dumps({\"location\": \"San Fransisco\", \"temperature\": \"15\", \"unit\": \"fahrenheit\"})\n",
    "    elif \"paris\" in location.lower():\n",
    "        return json.dumps({\"location\": \"paris\", \"temperature\": \"18\", \"unit\": \"celcuis\"})\n",
    "    else:\n",
    "        return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.chat.chat_completion import ChatCompletionMessage, ChatCompletion\n",
    "\n",
    "def run_conversation(main_request: str)->str:\n",
    "    # step-1: send conversation and available functions to the model\n",
    "    messages = [{\"role\": \"user\", \"content\": main_request}]\n",
    "\n",
    "    # functions details\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\":\"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_current_weather\",\n",
    "                # description is important because NLP understands the context of question by reading it\n",
    "                \"description\": \"Get the current weather in a given location\",\n",
    "                # parametres required by \"get_current_weather\" function\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city and the state, e.g. San Fransisco, CA\",\n",
    "                        },\n",
    "                        \"Unit\": {\"type\":\"string\", \"enum\":[\"celsius\", \"fahrenheit\"]},\n",
    "                    },\n",
    "                    \"required\": [\"location\"],\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # first request\n",
    "    response: ChatCompletion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",  \n",
    "        messages=messages,\n",
    "        tools=tools,        # for function calling we require tools\n",
    "        tool_choice=\"auto\"  # auto is default, but we will be explicit\n",
    "    )\n",
    "\n",
    "    response_message: ChatCompletionMessage = response.choices[0].message\n",
    "    display(\"* First Response: \", dict(response_message))\n",
    "\n",
    "    tool_calls = response_message.tool_calls    # will contain all the functions that are to be called\n",
    "    display(\"*First Response Tool Calls: \", list(tool_calls))\n",
    "\n",
    "    # step-2: check if the model wanted to call a function\n",
    "\n",
    "    if tool_calls:\n",
    "        # step-3: call the function\n",
    "        # Note: the JSON response may not always be valid; be sure to handle the erros\n",
    "        available_functions = {\n",
    "            \"get_current_weather\":get_current_weather   # only one function in this example, can be multiple\n",
    "        }\n",
    "\n",
    "    messages.append(response_message)   # extends conversation with assistsnt's resply\n",
    "\n",
    "    # srep-4: send the info for each function call and function response to the model\n",
    "\n",
    "    for tool_call in tool_calls:\n",
    "        function_name = tool_call.function.name\n",
    "        function_to_call = available_functions[function_name]\n",
    "        function_args = json.loads(tool_call.function.arguments)\n",
    "        function_response = function_to_call(\n",
    "            location = function_args.get(\"location\"),\n",
    "            unit=function_args.get(\"unit\")\n",
    "        )\n",
    "        messages.append(\n",
    "            {\n",
    "                \"tool_call_id\": tool_call.id,\n",
    "                \"role\": \"tool\",\n",
    "                \"name\": function_name,\n",
    "                \"content\": function_response\n",
    "            }\n",
    "        )   # extend conversation with function response\n",
    "\n",
    "    display(\"* Second Resonse Messages: \", list(messages))\n",
    "\n",
    "    second_response: ChatCompletion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "        messages=messages,\n",
    "    )   # Get a new response from the model where it can see the function response\n",
    "\n",
    "    print(\"* Second Response: \", dict(second_response))\n",
    "\n",
    "    return second_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_conversation(\"What is current weather in tokio?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
