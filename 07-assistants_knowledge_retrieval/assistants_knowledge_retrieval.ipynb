{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PostGreSQL data base\n",
    "# Azure\n",
    "# Google\n",
    "# AWS\n",
    "# Dockers\n",
    "# Terraform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assistant API- Knowledge Retrieval\n",
    "* External information can be provided to LLM\n",
    "    * Fine Tune: Teaching the LLM new information (very expensve & requires expert skills)\n",
    "    * Knowledge Retrieval: External informaion is converted into embeddings and saving in vector data bases. And embeddings related to the questions are provided in responses.\n",
    "    * Function Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Providing information to LLM \n",
    "* A file can be uploaded to LLM to 2 levels\n",
    "    * At Assistant Level: Publically available and can be accessed by other assistants.\n",
    "    * At Thread Level: Available only at particulat Thread Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "_ : bool = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client : OpenAI = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-1: Upload a file and create assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.beta import Assistant\n",
    "\n",
    "# upload a file with an \"assistant's\" purpose\n",
    "# OpenAI will do upload, chunk, index, embedd, vector search and retrieve content from the file.\n",
    "\n",
    "file = client.files.create(\n",
    "    file = open(\"zia_profile.pdf\", \"rb\"),\n",
    "    purpose= \"assistants\"\n",
    ")\n",
    "print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instruction to the assistant is general purpose and will be applicable to all the messages to this assistant\n",
    "\n",
    "assistant : Assistant = client.beta.assistants.create(\n",
    "    name=\"Student Support Assistant\",\n",
    "    instructions=\"You are astudent support chatbot. Use your knowledge base to best respond to student queries.\",\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    tools=[{\"type\": \"retrieval\"}],\n",
    "    file_ids=[file.id]      # connects file with this assistant\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-2: Create a Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.beta.thread import Thread\n",
    "\n",
    "thread : Thread = client.beta.threads.create()\n",
    "\n",
    "print(thread)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-3: Add a message to thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.beta.threads.thread_message import ThreadMessage\n",
    "\n",
    "messsage = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"When and which city Zia U. Khan was born?\" # LLM will retrieve this information from our uploaded file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-4: Run the Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instructions to run are optional and specific to this task only\n",
    "\n",
    "from openai.types.beta.threads.run import Run\n",
    "\n",
    "run : Run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id,\n",
    "    instructions=\"Please address the user as Pakistani. The user is a student of PIAIC.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-5: Check the Run Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run : Run = client.beta.threads.runs.retrieve(\n",
    "    thread_id=thread.id,\n",
    "    run_id=run.id,\n",
    ")\n",
    "\n",
    "print(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-6: Display the Assistant's Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages : list[ThreadMessage] = client.beta.threads.messages.list(\n",
    "    thread_id=thread.id,\n",
    ")\n",
    "\n",
    "for m in reversed(messages.data):\n",
    "    print(m.role + \": \" + m.content[0].text.value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
